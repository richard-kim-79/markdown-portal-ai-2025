import feedparser
import requests
from trafilatura import extract
from summarizer.summarizer import summarize_documents
from summarizer.tagger import generate_tags
from summarizer.markdown_generator import save_markdown
from db.models import insert_document


def load_rss_feeds(path="crawler/rss_feeds.txt"):
    with open(path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]


def fetch_and_process_feed(feed_url):
    feed = feedparser.parse(feed_url)
    for entry in feed.entries[:3]:  # ê° í”¼ë“œë‹¹ ìµœê·¼ 3ê°œ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
        title = entry.title
        url = entry.link

        print(f"ğŸ”— {title} ({url})")

        try:
            html = requests.get(url, timeout=10).text
            content = extract(html)
            if not content:
                print("âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"); continue

            summary = summarize_documents(content)
            tags = generate_tags(summary)
            save_path = save_markdown(title, summary, tags, content)

            insert_document(title, content, summary, tags, category="ë‰´ìŠ¤", url=url, saved_path=str(save_path))
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


def run_rss_crawler():
    feeds = load_rss_feeds()
    print(f"ğŸŒ RSS í”¼ë“œ {len(feeds)}ê°œ ìˆ˜ì§‘ ì‹œì‘...")
    for feed_url in feeds:
        fetch_and_process_feed(feed_url)
    print("ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")


if __name__ == "__main__":
    run_rss_crawler()
